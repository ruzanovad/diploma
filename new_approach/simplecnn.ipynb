{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import bounding_box\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class LatexDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(\".png\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        label_path = os.path.join(self.label_dir, img_name.replace(\".png\", \".txt\"))\n",
    "\n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = image.shape  # Original image size\n",
    "\n",
    "        # Load label\n",
    "        with open(label_path, \"r\") as f:\n",
    "            label_data = [list(map(float, line.split())) for line in f.readlines()]\n",
    "        \n",
    "        class_labels = []\n",
    "        bboxes = []\n",
    "\n",
    "        for label in label_data:\n",
    "            class_id = int(label[0])\n",
    "            x_center, y_center, width, height = label[1:]\n",
    "\n",
    "            x_min = (x_center - width / 2) \n",
    "            y_min = (y_center - height / 2)\n",
    "            x_max = (x_center + width / 2) \n",
    "            y_max = (y_center + height / 2)\n",
    "\n",
    "            class_labels.append(class_id)\n",
    "            bboxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        # Convert to tensors\n",
    "        image = cv2.resize(image, (128, 128)) / 255.0 \n",
    "        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)  \n",
    "        class_labels = torch.tensor(class_labels, dtype=torch.long)\n",
    "        bboxes = torch.tensor(bboxes, dtype=torch.float32)\n",
    "\n",
    "        return image, class_labels, bboxes\n",
    "    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length bounding boxes.\n",
    "    \"\"\"\n",
    "    images, class_labels, bboxes = zip(*batch)  # Unpack batch\n",
    "    images = torch.stack(images, dim=0)  # Stack images normally\n",
    "\n",
    "    return images, list(class_labels), list(bboxes)  # Keep labels & bboxes as lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "max_objects = 100\n",
    "image_dir = \"dataset\"\n",
    "label_dir = \"dataset\"\n",
    "train_dataset = LatexDataset(image_dir, label_dir, transform=None)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectDetectorSimpleCNN(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=65536, out_features=256, bias=True)\n",
       "  (fc_class): Linear(in_features=256, out_features=1000, bias=True)\n",
       "  (fc_bbox): Linear(in_features=256, out_features=400, bias=True)\n",
       "  (fc_confidence): Linear(in_features=256, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ObjectDetectorSimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes, max_objects):\n",
    "        super(ObjectDetectorSimpleCNN, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.max_objects = max_objects\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 256)\n",
    "\n",
    "        # Class prediction head\n",
    "        self.fc_class = nn.Linear(256, num_classes*max_objects)\n",
    "\n",
    "        # Bounding Box prediction head (x_min, y_min, x_max, y_max)\n",
    "        self.fc_bbox = nn.Linear(256, 4*max_objects)\n",
    "\n",
    "        self.fc_confidence = nn.Linear(256, max_objects)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], -1)  # Flatten\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        class_output = self.fc_class(x).view(x.shape[0], self.max_objects, self.num_classes)  # [batch_size, max_objects, num_classes]\n",
    "        bbox_output = self.fc_bbox(x).view(x.shape[0], self.max_objects, 4)  # [batch_size, max_objects, 4]\n",
    "       \n",
    "        confidence_output = torch.sigmoid(self.fc_confidence(x)) # 0 to 1 range\n",
    "\n",
    "        return class_output, bbox_output, confidence_output\n",
    "\n",
    "\n",
    "model = ObjectDetectorSimpleCNN(len(bounding_box.types), max_objects)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "bbox_loss_fn = nn.MSELoss()\n",
    "confidence_loss_fn = nn.BCELoss()  # Confidence loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Class Loss: 647.2732, BBox Loss: 53839.6027, Conf Loss: 83.0674\n",
      "Epoch 2, Class Loss: 487.6732, BBox Loss: 29922.0896, Conf Loss: 0.0010\n",
      "Epoch 3, Class Loss: 375.0172, BBox Loss: 28108.3630, Conf Loss: 0.0000\n",
      "Epoch 4, Class Loss: 308.6285, BBox Loss: 27451.3144, Conf Loss: 0.0000\n",
      "Epoch 5, Class Loss: 276.0655, BBox Loss: 26311.0074, Conf Loss: 0.0000\n",
      "Epoch 6, Class Loss: 250.5802, BBox Loss: 24772.5605, Conf Loss: 0.0000\n",
      "Epoch 7, Class Loss: 295.2268, BBox Loss: 23746.7567, Conf Loss: 0.0000\n",
      "Epoch 8, Class Loss: 360.8060, BBox Loss: 21739.9703, Conf Loss: 0.0000\n",
      "Epoch 9, Class Loss: 354.2438, BBox Loss: 18881.0616, Conf Loss: 0.0000\n",
      "Epoch 10, Class Loss: 288.0420, BBox Loss: 15933.9975, Conf Loss: 0.0000\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_class_loss, running_bbox_loss, running_conf_loss = 0.0, 0.0, 0.0\n",
    "\n",
    "    for images, class_labels, bboxes in train_loader:\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        class_preds, bbox_preds, confidence_preds = model(images)\n",
    "\n",
    "        loss_class, loss_bbox, loss_conf = 0, 0, 0\n",
    "\n",
    "        for i in range(len(class_labels)):  # Loop over batch\n",
    "            cls_target = class_labels[i].to(device)\n",
    "            bbox_target = bboxes[i].to(device)\n",
    "\n",
    "            num_objects = cls_target.shape[0]\n",
    "            pred_classes = class_preds[i][:num_objects, :]\n",
    "            pred_bboxes = bbox_preds[i][:num_objects, :]\n",
    "            pred_confidences = confidence_preds[i][:num_objects]\n",
    "\n",
    "            loss_class += class_loss_fn(pred_classes, cls_target)\n",
    "            loss_bbox += bbox_loss_fn(pred_bboxes, bbox_target)\n",
    "            loss_conf += confidence_loss_fn(pred_confidences, torch.ones(num_objects, device=device))\n",
    "\n",
    "        loss = loss_class + loss_bbox + loss_conf\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_class_loss += loss_class.item()\n",
    "        running_bbox_loss += loss_bbox.item()\n",
    "        running_conf_loss += loss_conf.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Class Loss: {running_class_loss:.4f}, BBox Loss: {running_bbox_loss:.4f}, Conf Loss: {running_conf_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
