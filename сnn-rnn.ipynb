{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "#  Допустим, у нас уже есть класс Dataset, который отдаёт:\n",
    "#    - image (тензор [C, H, W])\n",
    "#    - tokens (тензор [T])\n",
    "# ----------------------------------------------------------------\n",
    "class FormulaDataset(Dataset):\n",
    "    def __init__(self, images, token_sequences, transform=None):\n",
    "        \"\"\"\n",
    "        images: список или массив (B, H, W, C) либо пути к изображениям\n",
    "        token_sequences: список тензоров или списков индексов (B, T)\n",
    "        transform: torchvision transforms для картинок, если нужно\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.token_sequences = token_sequences\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]  # здесь может быть путь к файлу, нужно загрузить + применить transform\n",
    "        tokens = self.token_sequences[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # например, Resize, ToTensor, нормализация\n",
    "        \n",
    "        # image: тензор [C, H, W]\n",
    "        # tokens: тензор [T]\n",
    "        return image, tokens\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "#  Коллатер (необязательно), если нужно паддить последовательности\n",
    "# ----------------------------------------------------------------\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch = [(image1, tokens1), (image2, tokens2), ...]\n",
    "    Надо привести к одинаковой длине последовательности токенов, \n",
    "    если они разной длины, плюс сформировать batch тензоров для картинок.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    tokens_list = []\n",
    "    lengths = []\n",
    "    \n",
    "    for (img, tok) in batch:\n",
    "        images.append(img)\n",
    "        tokens_list.append(tok)\n",
    "        lengths.append(tok.size(0))\n",
    "    \n",
    "    # Допустим, хотим паддить по максимальной длине в батче\n",
    "    max_len = max(lengths)\n",
    "    \n",
    "    # Паддим последовательности нулями (или <PAD>-токеном, если у вас в словаре есть такой ID)\n",
    "    padded_tokens = []\n",
    "    for tok in tokens_list:\n",
    "        pad_size = max_len - tok.size(0)\n",
    "        if pad_size > 0:\n",
    "            # дополним нулями в конце\n",
    "            pad = torch.zeros(pad_size, dtype=tok.dtype)\n",
    "            # или tok.new_full((pad_size,), fill_value=PAD_ID)\n",
    "            tok = torch.cat([tok, pad], dim=0)\n",
    "        padded_tokens.append(tok.unsqueeze(0))\n",
    "    \n",
    "    padded_tokens = torch.cat(padded_tokens, dim=0)  # [B, max_len]\n",
    "    \n",
    "    # Склеим изображения в один тензор [B, C, H, W]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    \n",
    "    return images, padded_tokens, lengths\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "#  Модель (упрощённый вариант, можно взять тот, что выше)\n",
    "# ----------------------------------------------------------------\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, input_channels=1, feature_dim=256):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(128, feature_dim, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, images):\n",
    "        # images: [B, C, H, W]\n",
    "        features = self.conv_layers(images)  # [B, feature_dim, H/4, W/4]\n",
    "        # Просто возьмём global average pooling как пример\n",
    "        # (или можно оставить spatial map, чтобы потом использовать attention)\n",
    "        pooled = features.mean(dim=[2, 3])  # [B, feature_dim]\n",
    "        return pooled\n",
    "\n",
    "\n",
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=256):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim + hidden_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        self.init_h = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.init_c = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, encoder_out, tokens):\n",
    "        \"\"\"\n",
    "        encoder_out: [B, hidden_dim] – вектор контекста от CNN\n",
    "        tokens: [B, T] – входные токены (teacher forcing)\n",
    "        \"\"\"\n",
    "        B, T = tokens.shape\n",
    "        \n",
    "        # начальные h, c\n",
    "        h0 = self.init_h(encoder_out).unsqueeze(0)  # [1, B, hidden_dim]\n",
    "        c0 = self.init_c(encoder_out).unsqueeze(0)  # [1, B, hidden_dim]\n",
    "        \n",
    "        # эмбеддинги токенов [B, T, embed_dim]\n",
    "        emb = self.embedding(tokens)\n",
    "        \n",
    "        # на каждом шаге конкатенируем (emb[t], encoder_out) по фичам\n",
    "        # для этого \"растянем\" encoder_out по длине T\n",
    "        context_expanded = encoder_out.unsqueeze(1).expand(-1, T, -1)  # [B, T, hidden_dim]\n",
    "        lstm_input = torch.cat([emb, context_expanded], dim=2)  # [B, T, embed_dim+hidden_dim]\n",
    "        \n",
    "        outputs, (hn, cn) = self.lstm(lstm_input, (h0, c0))  # [B, T, hidden_dim]\n",
    "        logits = self.fc_out(outputs)  # [B, T, vocab_size]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class Image2LatexModel(nn.Module):\n",
    "    def __init__(self, vocab_size, \n",
    "                 cnn_input_channels=1, cnn_feature_dim=256,\n",
    "                 embed_dim=256, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = CNNEncoder(cnn_input_channels, cnn_feature_dim)\n",
    "        self.decoder = RNNDecoder(vocab_size, embed_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, images, tokens):\n",
    "        encoder_out = self.encoder(images)     # [B, feature_dim]\n",
    "        logits = self.decoder(encoder_out, tokens)  # [B, T, vocab_size]\n",
    "        return logits\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "#  Пример обучения\n",
    "# ----------------------------------------------------------------\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images, tokens, lengths in dataloader:\n",
    "        images = images.to(device)\n",
    "        tokens = tokens.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(images, tokens)  # [B, T, vocab_size]\n",
    "        \n",
    "        # Нам надо сравнить logits с \"истинными\" токенами. \n",
    "        # Предположим, что в tokens как раз записаны \"таргеты\".\n",
    "        # Но учтите, что иногда токен на входе — это shift-на-1 (без последнего),\n",
    "        # а таргет — сам реальный. Для упрощённого примера используем один и тот же.\n",
    "        \n",
    "        # logits: [B, T, vocab_size]\n",
    "        # tokens: [B, T]\n",
    "        \n",
    "        # Перегоняем всё в 2D для CrossEntropyLoss:\n",
    "        B, T, V = logits.shape\n",
    "        logits_2d = logits.view(B*T, V)       # [B*T, V]\n",
    "        targets_2d = tokens.view(B*T)         # [B*T]\n",
    "        \n",
    "        loss = criterion(logits_2d, targets_2d)\n",
    "        \n",
    "        # Обратный проход\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, tokens, lengths in dataloader:\n",
    "            images = images.to(device)\n",
    "            tokens = tokens.to(device)\n",
    "            \n",
    "            logits = model(images, tokens)\n",
    "            B, T, V = logits.shape\n",
    "            logits_2d = logits.view(B*T, V)\n",
    "            targets_2d = tokens.view(B*T)\n",
    "            \n",
    "            loss = criterion(logits_2d, targets_2d)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Параметры\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vocab_size = 100  # допустим, у нас словарь из 100 токенов\n",
    "    batch_size = 8\n",
    "    num_epochs = 5\n",
    "    \n",
    "    # Сформируем фиктивный датасет (для примера)\n",
    "    # На практике вы используете свои изображения + токены\n",
    "    train_images = [torch.randn(1, 64, 64) for _ in range(100)]  # 1-канальные изображения\n",
    "    train_tokens = [torch.randint(0, vocab_size, (10,)) for _ in range(100)]\n",
    "    \n",
    "    val_images = [torch.randn(1, 64, 64) for _ in range(20)]\n",
    "    val_tokens = [torch.randint(0, vocab_size, (10,)) for _ in range(20)]\n",
    "    \n",
    "    train_dataset = FormulaDataset(train_images, train_tokens)\n",
    "    val_dataset = FormulaDataset(val_images, val_tokens)\n",
    "    \n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                            shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # Модель\n",
    "    model = Image2LatexModel(vocab_size=vocab_size, \n",
    "                             cnn_input_channels=1,\n",
    "                             cnn_feature_dim=256,\n",
    "                             embed_dim=256,\n",
    "                             hidden_dim=256)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Оптимизатор и функция потерь\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # допустим, 0 = PAD токен\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
